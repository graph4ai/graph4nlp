import copy
import warnings

import numpy as np
import torch

from graph4nlp.pytorch.modules.evaluation.base import EvaluationMetricBase


class Accuracy(EvaluationMetricBase):
    """
        Calculate precision, recall, F1 for each labels
    """

    def __init__(self, metrics):
        """
            The initial function for this class
        Parameters
        ----------
        metrics: list
            Indicate the metric for the class to return.
            Note that each metric must be one of ``precision``, ``recall``, ``F1``
        """
        super(Accuracy, self).__init__()
        if not isinstance(metrics, list):
            for metric in metrics:
                if metric not in ["precision", "recall", "F1"]:
                    raise TypeError("argument metric must be list of str containing 'precision', 'recall', 'F1'")
        self.metrics = metrics

    def calculate_scores(self, ground_truth, predict, average=None, zero_division="warning", sample_weight=None):
        """
            The function to calculate the expected metrics for each labels
        Parameters
        ----------
        ground_truth: torch.Tensor
            Ground truth (correct) target values, 1d tensor
        predict: torch.Tensor
            The predicted target values generated by classifier, 1d tensor
        average: string or None, [None (default), 'micro', 'macro', 'weighted']
            If set ``None``, it will return the scores for each class. Otherwise, it will be reduced \
                by the strategy as follows:

            ``'micro'``:
                Calculate metrics globally by counting the total true positives,
                false negatives and false positives.

            ``'macro'``:
                Calculate metrics for each label, and calculate the unweighted
                average values.  This does not take label imbalance into account.

            ``'weighted'``:
                Calculate metrics for each label, and calculate the weighted
                average value. Note that the weight is the number of the true
                instances for each label.

        zero_division: "warning", 0, 1, default="warning"
                Sets the value to return when there is a zero division.

                If set to "warning", this acts as 0, but warnings are also raised.

        sample_weight: None
            The sample weight. It is not implemented yet.

        Returns
        -------
        scores: list[object]
            Return the expected metrics initialized in init function in ``precision``, ``recall``, ``F1`` order
        """
        ground_truth_np, predict_np = self._check_available(ground_truth, predict, zero_division)
        mcm = self._calculate_confusion_matrix(ground_truth=ground_truth_np, predict=predict_np)

        tp_sum = mcm[:, 1, 1]
        pred_sum = tp_sum + mcm[:, 0, 1]
        gt_sum = tp_sum + mcm[:, 1, 0]

        if average == 'micro':
            tp_sum = np.array([tp_sum.sum()])
            pred_sum = np.array([pred_sum.sum()])
            gt_sum = np.array([gt_sum.sum()])

        # calculate precision and recall
        precision = self._prf_divide(tp_sum, pred_sum, zero_division=zero_division)
        recall = self._prf_divide(tp_sum, gt_sum, zero_division=zero_division)

        # calculate F_beta
        beta2 = 1 ** 2  # note: only F1 here
        denominator = beta2 * precision + recall

        denominator[denominator == 0.] = 1  # avoid division by 0
        f_score = (1 + beta2) * precision * recall / denominator

        if average == "weighted":
            weighted = gt_sum
        else:
            weighted = None

        if average is not None:
            precision = np.average(precision, weights=weighted)
            recall = np.average(recall, weights=weighted)
            f_score = np.average(f_score, weights=weighted)
        scores = []
        if "precision" in self.metrics:
            scores.append(precision)
        if "recall" in self.metrics:
            scores.append(recall)
        if "F1" in self.metrics:
            scores.append(f_score)
        return scores

    @staticmethod
    def _prf_divide(numerator, denominator, zero_division):
        """
            The function performs division and handles zero-division situations.
        Parameters
        ----------
        numerator: numpy.ndarray
        denominator: numpy.ndarray
        zero_division: "warning", 0, 1, default="warning"
            Sets the value to return when there is a zero division.

            If set to "warning", this acts as 0, but warnings are also raised.
        Returns
        -------
        results: numpy.ndarray
            The division results.

        """
        zero_mask = denominator == 0.
        denominator_cp = copy.deepcopy(denominator)
        denominator_cp[zero_mask] = 1.
        ret = numerator / denominator_cp
        if np.sum(zero_mask) == 0:
            return ret
        ret[zero_mask] = 0. if zero_division in ["warning", 0] else 1.
        if zero_division is "warning":
            warnings.warn("zero division encountered")
        return ret

    @staticmethod
    def _check_available(ground_truth, predict, zero_division):
        """
            The function to check the parameters.
            If all tests are passed, it will convert the tensor to numpy.
        Parameters
        ----------
        ground_truth: Any
        predict: Any
        zero_division: Any

        Returns
        -------
        ground_truth: numpy.ndarray
            numpy version of tensor ground_truth
        predict: numpy.ndarray
            numpy version of tensor predict

        Raises
        -------
        TypeError: TypeError
        ValueError: ValueError
        """
        if not isinstance(ground_truth, torch.Tensor):
            raise TypeError("argument ground_truth must be torch.tensor")
        if not isinstance(predict, torch.Tensor):
            raise TypeError("argument predict must be torch.tensor")
        if ground_truth.dtype not in [torch.int, torch.int8, torch.int16, torch.int32, torch.int64]:
            raise TypeError("argument ground_truth must be int tensor")
        if predict.dtype not in [torch.int, torch.int8, torch.int16, torch.int32, torch.int64]:
            raise TypeError("argument predict must be int tensor")
        if len(ground_truth.shape) != 1:
            raise TypeError("argument ground_truth must be 1d tensor")
        if len(predict.shape) != 1:
            raise TypeError("argument predict must be 1d tensor")
        if ground_truth.shape[0] != predict.shape[0]:
            raise ValueError("argument ground_truth and predict must be the same shape")

        zero_division_ok = False
        if isinstance(zero_division, str) and zero_division is "warning":
            zero_division_ok = True
        elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:
            zero_division_ok = True

        if not zero_division_ok:
            raise ValueError("argument zero_division must be in ['warning', 0, 1]")

        return ground_truth.numpy(), predict.numpy()

    def _calculate_confusion_matrix(self, ground_truth, predict):
        """
            The function to calculate the confusion matrix for multi-class inputs.
            The labels will be selected and transformed. eg: [1, 2, 3] --> [0, 1, 2]

            In multi-class confusion matrix :math:`MCM`, the count of true negatives is
            :math:`MCM_{:,0,0}`, false positives is :math:`MCM_{:,0,1}`, false negatives
            is :math:`MCM_{:,1,0}` and true positive is :math:`MCM_{:,1,1}`.

        Parameters
        ----------
        ground_truth: numpy.ndarray
        predict: numpy.ndarray

        Returns
        -------
        confusion_matrix: numpy.ndarray
            The confusion matrix which has the shape: [num_labels, 2, 2]
        """
        unique_labels = sorted(self._get_unique_labels(ground_truth, predict))
        ground_truth_transformed = np.searchsorted(unique_labels, ground_truth)
        predict_transformed = np.searchsorted(unique_labels, predict)
        n_labels = len(unique_labels)

        tp = ground_truth_transformed == predict_transformed
        tp_bins = ground_truth_transformed[tp]
        tp_sum = np.bincount(tp_bins, weights=None, minlength=n_labels)
        pred_sum = np.bincount(predict_transformed, minlength=n_labels)
        gt_sum = np.bincount(ground_truth_transformed, minlength=n_labels)
        fp = pred_sum - tp_sum
        fn = gt_sum - tp_sum
        tp = tp_sum
        tn = ground_truth_transformed.shape[0] - tp - fp - fn
        return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)

    @staticmethod
    def _get_unique_labels(*lists):
        """
            find the unique elements in the given lists
        Parameters
        ----------
        lists: [numpy.ndarray]
            List of lists which contain labels.
        Returns
        -------
        unique_labels: numpy.ndarray
            It has unique labels encountered in the ``lists``.
        """
        ret = []
        for li in lists:
            unique_li = np.unique(li)
            ret.extend(unique_li.tolist())
        ret = list(set(ret))
        return np.array(ret)


if __name__ == "__main__":
    # test mcm component
    from sklearn import metrics
    import tqdm

    # ground_truth = np.array([0, 1, 3, 1, 2])
    # predict = np.array([0, 2, 3, 3, 1])
    ground_truth = np.array([0, 1, 3, 1, 1])
    predict = np.array([0, 1, 3, 3, 1])
    mcm_sklearn = metrics.multilabel_confusion_matrix(ground_truth, predict)
    accuracy = Accuracy(metrics=["precision", "recall", "F1"])
    mcm_graph4ai = accuracy._calculate_confusion_matrix(ground_truth=ground_truth, predict=predict)
    assert (mcm_graph4ai == mcm_sklearn).all()
    print("mcm special case test passed")

    # test precision, recall, F1
    precision_sklearn, recall_sklearn, f1_sklearn, _ = metrics.precision_recall_fscore_support(y_true=ground_truth,
                                                                                               y_pred=predict,
                                                                                               average="micro")

    precision_graph4ai, recall_graph4ai, f1_graph4ai = accuracy.calculate_scores(
        ground_truth=torch.from_numpy(ground_truth), predict=torch.from_numpy(predict),
        average="micro")
    assert precision_graph4ai == precision_sklearn
    assert recall_graph4ai == recall_sklearn
    assert f1_graph4ai == f1_sklearn
    print("micro average special case test passed")

    precision_sklearn, recall_sklearn, f1_sklearn, _ = metrics.precision_recall_fscore_support(y_true=ground_truth,
                                                                                               y_pred=predict,
                                                                                               average="macro")

    precision_graph4ai, recall_graph4ai, f1_graph4ai = accuracy.calculate_scores(
        ground_truth=torch.from_numpy(ground_truth), predict=torch.from_numpy(predict),
        average="macro")
    assert precision_graph4ai == precision_sklearn
    assert recall_graph4ai == recall_sklearn
    assert f1_graph4ai == f1_sklearn
    print("macro average special case test passed")
    precision_sklearn, recall_sklearn, f1_sklearn, _ = metrics.precision_recall_fscore_support(y_true=ground_truth,
                                                                                               y_pred=predict,
                                                                                               average="weighted")

    precision_graph4ai, recall_graph4ai, f1_graph4ai = accuracy.calculate_scores(
        ground_truth=torch.from_numpy(ground_truth), predict=torch.from_numpy(predict),
        average="weighted")
    assert precision_graph4ai == precision_sklearn
    assert recall_graph4ai == recall_sklearn
    assert f1_graph4ai == f1_sklearn
    print("weighted average special case test passed")

    # random test
    import random

    for test_time in tqdm.tqdm(range(100)):
        length = random.randint(2, 1000000)
        n_labels = random.randint(2, 100000)
        ground_truth = []
        predict = []
        for i in range(length):
            ground_truth.append(random.randint(0, n_labels - 1))
            predict.append(random.randint(0, n_labels - 1))
        ground_truth = np.array(ground_truth)
        predict = np.array(predict)
        average = random.choice(["weighted", "macro", "micro"])
        precision_sklearn, recall_sklearn, f1_sklearn, _ = metrics.precision_recall_fscore_support(y_true=ground_truth,
                                                                                                   y_pred=predict,
                                                                                                   average=average)

        precision_graph4ai, recall_graph4ai, f1_graph4ai = accuracy.calculate_scores(
            ground_truth=torch.from_numpy(ground_truth), predict=torch.from_numpy(predict),
            average=average)
        assert precision_graph4ai == precision_sklearn
        assert recall_graph4ai == recall_sklearn
        assert f1_graph4ai == f1_sklearn
    print("random test passed")
    # random test
    import random

    for test_time in tqdm.tqdm(range(100)):
        length = random.randint(2, 1000000)
        n_labels = random.randint(2, 100000)
        replace_ratio = random.random() / 1.5
        ground_truth = []
        predict = []
        for i in range(length):
            ground_truth.append(random.randint(0, n_labels - 1))
            if random.random() < replace_ratio:
                predict.append(random.randint(0, n_labels - 1))
            else:
                predict.append(ground_truth[-1])
        ground_truth = np.array(ground_truth)
        predict = np.array(predict)
        average = random.choice(["weighted", "macro", "micro"])
        precision_sklearn, recall_sklearn, f1_sklearn, _ = metrics.precision_recall_fscore_support(y_true=ground_truth,
                                                                                                   y_pred=predict,
                                                                                                   average=average)

        precision_graph4ai, recall_graph4ai, f1_graph4ai = accuracy.calculate_scores(
            ground_truth=torch.from_numpy(ground_truth), predict=torch.from_numpy(predict),
            average=average)
        assert precision_graph4ai == precision_sklearn
        assert recall_graph4ai == recall_sklearn
        assert f1_graph4ai == f1_sklearn
    print("random test passed")

    '''
    testing output
    
    ssh://shiina@10.214.223.119:22/home/shiina/env/anaconda3/envs/torch/bin/python -u /home/shiina/shiina/lib/graph4nlp/graph4nlp/pytorch/modules/evaluation/accuracy.py
    mcm special case test passed
    macro average special case test passed
    weighted average special case test passed
      1%|▍                                          | 1/100 [00:00<00:49,  1.99it/s]/home/shiina/env/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
      _warn_prf(average, modifier, msg_start, len(result))
    /home/shiina/env/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
      _warn_prf(average, modifier, msg_start, len(result))
    /home/shiina/shiina/lib/graph4nlp/graph4nlp/pytorch/modules/evaluation/accuracy.py:85: UserWarning: zero division encountered
      warnings.warn("zero division encountered")
    100%|█████████████████████████████████████████| 100/100 [03:01<00:00,  1.82s/it]
    random test passed
    100%|█████████████████████████████████████████| 100/100 [03:09<00:00,  1.89s/it]
    random test passed
    
    Process finished with exit code 0

    
    '''
