{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5780bc73",
   "metadata": {},
   "source": [
    "# Demo: Knowledge Graph Completion\n",
    "\n",
    "In this tutorial demo, we will use the Graph4NLP library to build a GNN-based knowledge graph completion model. The model consists of\n",
    "\n",
    "+ graph embedding module (e.g., Bi-Sep GGNN)\n",
    "+ predictoin module (e.g., DistMult decoder)\n",
    "\n",
    "We will use the built-in Graph2Seq model APIs to build the model, and evaluate it on the Kinship dataset.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "1. Create virtual environment\n",
    "\n",
    "+ conda create --name graph4nlp python=3.7\n",
    "+ conda activate graph4nlp\n",
    "\n",
    "2. Install graph4nlp library\n",
    "\n",
    "+ Clone the github repo\n",
    "\n",
    "```\n",
    "git clone -b stable https://github.com/graph4ai/graph4nlp.git\n",
    "cd graph4nlp\n",
    "```\n",
    "\n",
    "+ Then run ./configure (or ./configure.bat if you are using Windows 10) to config your installation. The configuration program will ask you to specify your CUDA version. If you do not have a GPU, please choose 'cpu'.\n",
    "\n",
    "```\n",
    "./configure\n",
    "```\n",
    "\n",
    "+ Finally, install the package\n",
    "\n",
    "```\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "3. Set up StanfordCoreNLP (for static graph construction only, unnecessary for this demo because preprocessed data is provided)\n",
    "\n",
    "+ Download StanfordCoreNLP\n",
    "+ Go to the root folder and start the server\n",
    "\n",
    "```\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e57576",
   "metadata": {},
   "source": [
    "# Installation and Date preprocessing for KGC\n",
    "+ Download the default English model used by **spaCy**, which is installed in the previous step \n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install h5py\n",
    "pip install future\n",
    "```\n",
    "+ Run the preprocessing script for WN18RR and Kinship: ```sh examples/pytorch/kg_completion/preprocess.sh```\n",
    "+ You can now run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d329cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from evaluation import ranking_and_hits\n",
    "from model import ConvE, Distmult, Complex, GGNNDistMult, GCNDistMult, GCNComplex\n",
    "\n",
    "from spodernet.preprocessing.pipeline import DatasetStreamer\n",
    "from spodernet.preprocessing.processors import JsonLoaderProcessors, Tokenizer, AddToVocab, SaveLengthsToState, StreamToHDF5, SaveMaxLengthsToState, CustomTokenizer\n",
    "from spodernet.preprocessing.processors import ConvertTokenToIdx, ApplyFunction, ToLower, DictKey2ListMapper, ApplyFunction, StreamToBatch\n",
    "from spodernet.utils.global_config import Config, Backends\n",
    "from spodernet.utils.logger import Logger, LogLevel\n",
    "from spodernet.preprocessing.batching import StreamBatcher\n",
    "from spodernet.preprocessing.pipeline import Pipeline\n",
    "from spodernet.preprocessing.processors import TargetIdx2MultiTarget\n",
    "from spodernet.hooks import LossHook, ETAHook\n",
    "from spodernet.utils.util import Timer\n",
    "from spodernet.preprocessing.processors import TargetIdx2MultiTarget\n",
    "import argparse\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b80903",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preprocess knowledge graph using spodernet. '''\n",
    "def preprocess(dataset_name, delete_data=False):\n",
    "    full_path = 'data/{0}/e1rel_to_e2_full.json'.format(dataset_name)\n",
    "    train_path = 'data/{0}/e1rel_to_e2_train.json'.format(dataset_name)\n",
    "    dev_ranking_path = 'data/{0}/e1rel_to_e2_ranking_dev.json'.format(dataset_name)\n",
    "    test_ranking_path = 'data/{0}/e1rel_to_e2_ranking_test.json'.format(dataset_name)\n",
    "\n",
    "    keys2keys = {}\n",
    "    keys2keys['e1'] = 'e1' # entities\n",
    "    keys2keys['rel'] = 'rel' # relations\n",
    "    keys2keys['rel_eval'] = 'rel' # relations\n",
    "    keys2keys['e2'] = 'e1' # entities\n",
    "    keys2keys['e2_multi1'] = 'e1' # entity\n",
    "    keys2keys['e2_multi2'] = 'e1' # entity\n",
    "    input_keys = ['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2']\n",
    "    d = DatasetStreamer(input_keys)\n",
    "    d.add_stream_processor(JsonLoaderProcessors())\n",
    "    d.add_stream_processor(DictKey2ListMapper(input_keys))\n",
    "\n",
    "    # process full vocabulary and save it to disk\n",
    "    d.set_path(full_path)\n",
    "    p = Pipeline(args.data, delete_data, keys=input_keys, skip_transformation=True)\n",
    "    p.add_sent_processor(ToLower())\n",
    "    p.add_sent_processor(CustomTokenizer(lambda x: x.split(' ')),keys=['e2_multi1', 'e2_multi2'])\n",
    "    p.add_token_processor(AddToVocab())\n",
    "    p.execute(d)\n",
    "    p.save_vocabs()\n",
    "\n",
    "\n",
    "    # process train, dev and test sets and save them to hdf5\n",
    "    p.skip_transformation = False\n",
    "    for path, name in zip([train_path, dev_ranking_path, test_ranking_path], ['train', 'dev_ranking', 'test_ranking']):\n",
    "        d.set_path(path)\n",
    "        p.clear_processors()\n",
    "        p.add_sent_processor(ToLower())\n",
    "        p.add_sent_processor(CustomTokenizer(lambda x: x.split(' ')),keys=['e2_multi1', 'e2_multi2'])\n",
    "        p.add_post_processor(ConvertTokenToIdx(keys2keys=keys2keys), keys=['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2'])\n",
    "        p.add_post_processor(StreamToHDF5(name, samples_per_file=1000, keys=input_keys))\n",
    "        p.execute(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c59c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, model_path):\n",
    "    if args.preprocess:\n",
    "        preprocess(args.data, delete_data=True)\n",
    "    input_keys = ['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2']\n",
    "    p = Pipeline(args.data, keys=input_keys)\n",
    "    p.load_vocabs()\n",
    "    vocab = p.state['vocab']\n",
    "\n",
    "    train_batcher = StreamBatcher(args.data, 'train', args.batch_size, randomize=True, keys=input_keys, loader_threads=args.loader_threads)\n",
    "    dev_rank_batcher = StreamBatcher(args.data, 'dev_ranking', args.test_batch_size, randomize=False, loader_threads=args.loader_threads, keys=input_keys)\n",
    "    test_rank_batcher = StreamBatcher(args.data, 'test_ranking', args.test_batch_size, randomize=False, loader_threads=args.loader_threads, keys=input_keys)\n",
    "\n",
    "\n",
    "    data = []\n",
    "    rows = []\n",
    "    columns = []\n",
    "    num_entities = vocab['e1'].num_token\n",
    "    num_relations = vocab['rel'].num_token\n",
    "\n",
    "    if args.preprocess:\n",
    "        for i, str2var in enumerate(train_batcher):\n",
    "            print(\"batch number:\", i)\n",
    "            for j in range(str2var['e1'].shape[0]):\n",
    "                for k in range(str2var['e2_multi1'][j].shape[0]):\n",
    "                    if str2var['e2_multi1'][j][k] != 0:\n",
    "                        data.append(str2var['rel'][j].cpu().tolist()[0])\n",
    "                        rows.append(str2var['e1'][j].cpu().tolist()[0])\n",
    "                        columns.append(str2var['e2_multi1'][j][k].cpu().tolist())\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        from graph4nlp.pytorch.data.data import GraphData, to_batch\n",
    "        KG_graph = GraphData()\n",
    "        KG_graph.add_nodes(num_entities)\n",
    "        for e1, rel, e2 in zip(rows, data, columns):\n",
    "            KG_graph.add_edge(e1, e2)\n",
    "            eid = KG_graph.edge_ids(e1, e2)[0]\n",
    "            KG_graph.edge_attributes[eid]['token'] = rel\n",
    "\n",
    "        torch.save(KG_graph, '{}/processed/KG_graph.pt'.format(args.data))\n",
    "        return\n",
    "\n",
    "\n",
    "    if args.model is None:\n",
    "        model = ConvE(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'conve':\n",
    "        model = ConvE(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'distmult':\n",
    "        model = Distmult(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'complex':\n",
    "        model = Complex(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'ggnn_distmult':\n",
    "        model = GGNNDistMult(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'gcn_distmult':\n",
    "        model = GCNDistMult(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'gcn_complex':\n",
    "        model = GCNComplex(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model!\")\n",
    "\n",
    "    if args.model in ['ggnn_distmult', 'gcn_distmult', 'gcn_complex']:\n",
    "        graph_path = '{}/processed/KG_graph.pt'.format(args.data)\n",
    "        KG_graph = torch.load(graph_path)\n",
    "        if Config.cuda:\n",
    "            KG_graph = KG_graph.to('cuda')\n",
    "    else:\n",
    "        KG_graph = None\n",
    "\n",
    "    train_batcher.at_batch_prepared_observers.insert(1,TargetIdx2MultiTarget(num_entities, 'e2_multi1', 'e2_multi1_binary'))\n",
    "\n",
    "    eta = ETAHook('train', print_every_x_batches=args.log_interval)\n",
    "    train_batcher.subscribe_to_events(eta)\n",
    "    train_batcher.subscribe_to_start_of_epoch_event(eta)\n",
    "    train_batcher.subscribe_to_events(LossHook('train', print_every_x_batches=args.log_interval))\n",
    "    if Config.cuda:\n",
    "        model.cuda()\n",
    "    if args.resume:\n",
    "        model_params = torch.load(model_path)\n",
    "        print(model)\n",
    "        total_param_size = []\n",
    "        params = [(key, value.size(), value.numel()) for key, value in model_params.items()]\n",
    "        for key, size, count in params:\n",
    "            total_param_size.append(count)\n",
    "            print(key, size, count)\n",
    "        print(np.array(total_param_size).sum())\n",
    "        model.load_state_dict(model_params)\n",
    "        model.eval()\n",
    "        ranking_and_hits(model, test_rank_batcher, vocab, 'test_evaluation', kg_graph=KG_graph)\n",
    "        ranking_and_hits(model, dev_rank_batcher, vocab, 'dev_evaluation', kg_graph=KG_graph)\n",
    "    else:\n",
    "        model.init()\n",
    "\n",
    "    total_param_size = []\n",
    "    params = [value.numel() for value in model.parameters()]\n",
    "    print(params)\n",
    "    print(np.sum(params))\n",
    "\n",
    "    best_mrr = 0\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        for i, str2var in enumerate(train_batcher):\n",
    "            opt.zero_grad()\n",
    "            e1 = str2var['e1']\n",
    "            rel = str2var['rel']\n",
    "            e2_multi = str2var['e2_multi1_binary'].float()\n",
    "            # label smoothing\n",
    "            e2_multi = ((1.0-args.label_smoothing)*e2_multi) + (1.0/e2_multi.size(1))\n",
    "\n",
    "            pred = model.forward(e1, rel, KG_graph)\n",
    "            loss = model.loss(pred, e2_multi)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_batcher.state.loss = loss.cpu()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % 2 == 0 and epoch > 0:\n",
    "                dev_mrr = ranking_and_hits(model, dev_rank_batcher, vocab, 'dev_evaluation', kg_graph=KG_graph)\n",
    "                if dev_mrr > best_mrr:\n",
    "                    best_mrr = dev_mrr\n",
    "                    print('saving best model to {0}'.format(model_path))\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            if epoch % 2 == 0:\n",
    "                if epoch > 0:\n",
    "                    ranking_and_hits(model, test_rank_batcher, vocab, 'test_evaluation', kg_graph=KG_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc00bb",
   "metadata": {},
   "source": [
    "### Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3eb791d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--kernel_size'], dest='kernel_size', nargs=None, const=None, default=5, type=<class 'int'>, choices=None, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Link prediction for knowledge graphs')\n",
    "parser.add_argument('--batch-size', type=int, default=128, help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=128, help='input batch size for testing/validation (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=1000, help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--lr', type=float, default=0.003, help='learning rate (default: 0.003)')\n",
    "parser.add_argument('--seed', type=int, default=1234, metavar='S', help='random seed (default: 17)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--data', type=str, default='kinship', help='Dataset to use: {FB15k-237, YAGO3-10, WN18RR, umls, nations, kinship}, default: FB15k-237')\n",
    "parser.add_argument('--l2', type=float, default=0.0, help='Weight decay value to use in the optimizer. Default: 0.0')\n",
    "parser.add_argument('--model', type=str, default='ggnn_distmult', help='Choose from: {conve, distmult, complex}')\n",
    "parser.add_argument('--direction_option', type=str, default='undirected', help='Choose from: {undirected, bi_sep, bi_fuse}')\n",
    "parser.add_argument('--embedding-dim', type=int, default=200, help='The embedding dimension (1D). Default: 200')\n",
    "parser.add_argument('--embedding-shape1', type=int, default=20, help='The first dimension of the reshaped 2D embedding. The second dimension is infered. Default: 20')\n",
    "parser.add_argument('--hidden-drop', type=float, default=0.25, help='Dropout for the hidden layer. Default: 0.3.')\n",
    "parser.add_argument('--input-drop', type=float, default=0.2, help='Dropout for the input embeddings. Default: 0.2.')\n",
    "parser.add_argument('--feat-drop', type=float, default=0.2, help='Dropout for the convolutional features. Default: 0.2.')\n",
    "parser.add_argument('--lr-decay', type=float, default=0.995, help='Decay the learning rate by this factor every epoch. Default: 0.995')\n",
    "parser.add_argument('--loader-threads', type=int, default=4, help='How many loader threads to use for the batch loaders. Default: 4')\n",
    "parser.add_argument('--preprocess', action='store_true', help='Preprocess the dataset. Needs to be executed only once. Default: 4')\n",
    "parser.add_argument('--resume', action='store_true', help='Resume a model.')\n",
    "parser.add_argument('--use-bias', action='store_true', help='Use a bias in the convolutional layer. Default: True')\n",
    "parser.add_argument('--label-smoothing', type=float, default=0.1, help='Label smoothing value to use. Default: 0.1')\n",
    "parser.add_argument('--hidden-size', type=int, default=9728, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).')\n",
    "\n",
    "parser.add_argument('--channels', type=int, default=200, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).')\n",
    "parser.add_argument('--kernel_size', type=int, default=5, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439d7fa",
   "metadata": {},
   "source": [
    "### If you run the task for the first time, run with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c42e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--data', 'kinship', '--model', 'ggnn_distmult', '--preprocess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eacc04a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107d1dcd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse console parameters and set global variables\n",
    "Config.backend = 'pytorch'\n",
    "Config.cuda = False\n",
    "Config.embedding_dim = args.embedding_dim\n",
    "\n",
    "model_name = '{2}_{0}_{1}'.format(args.input_drop, args.hidden_drop, args.model)\n",
    "model_path = 'saved_models/{0}_{1}.model'.format(args.data, model_name)\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535477d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee6ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041aa32c",
   "metadata": {},
   "source": [
    "### After preprocess the kinship data, then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8578e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-28 14:46:54.305167 (WARNING): delete_all_previous_data=True! Deleting all folder contents of folder /Users/gaohanning/.data/kinship!\n",
      "2021-07-28 14:46:54.336047 (INFO): Recreating path: /Users/gaohanning/.data/kinship\n",
      "2021-07-28 14:46:54.418709 (INFO): Time taken for 10000 samples for input type rel for processor AddToVocab: 0.01 seconds\n",
      "2021-07-28 14:46:54.523750 (INFO): Time taken for 10000 samples for input type e2 for processor ToLower: 0.01 seconds\n",
      "2021-07-28 14:46:54.524173 (INFO): Time taken for 10000 samples for input type e2 for processor SaveLengthsToState: 0.03 seconds\n",
      "2021-07-28 14:46:54.769508 (INFO): Time taken for 10000 samples for input type e2_multi2 for processor CustomTokenizer: 0.01 seconds\n",
      "2021-07-28 14:46:54.829736 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab\n",
      "2021-07-28 14:46:54.830481 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e1\n",
      "2021-07-28 14:46:54.830997 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_rel\n",
      "2021-07-28 14:46:54.831475 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_rel_eval\n",
      "2021-07-28 14:46:54.831996 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e2\n",
      "2021-07-28 14:46:54.832438 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e2_multi1\n",
      "2021-07-28 14:46:54.832864 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e2_multi2\n",
      "2021-07-28 14:46:54.940944 (INFO): Time taken for 10000 samples for input type e2 for processor ToLower: 0.01 seconds\n",
      "2021-07-28 14:46:54.941110 (INFO): Time taken for 10000 samples for input type e2 for processor SaveLengthsToState: 0.03 seconds\n",
      "2021-07-28 14:46:55.253799 (INFO): Time taken for 10000 samples for input type e2 for processor ConvertTokenToIdx: 0.03 seconds\n",
      "2021-07-28 14:46:55.253959 (INFO): Time taken for 10000 samples for input type e2 for processor StreamToHDF5: 0.09 seconds\n",
      "2021-07-28 14:46:55.275933 (INFO): Time taken for 10000 samples for input type e2_multi2 for processor CustomTokenizer: 0.01 seconds\n",
      "2021-07-28 14:46:56.010986 (WARNING): Pipeline path /Users/gaohanning/.data/kinship already exist. This pipeline may overwrite data in this path!\n",
      "2021-07-28 14:46:56.013233 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab\n",
      "2021-07-28 14:46:56.014121 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e1\n",
      "2021-07-28 14:46:56.014787 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel\n",
      "2021-07-28 14:46:56.015429 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel_eval\n",
      "2021-07-28 14:46:56.016041 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2\n",
      "2021-07-28 14:46:56.016506 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi1\n",
      "2021-07-28 14:46:56.016926 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi2\n",
      "batch number: 0\n",
      "batch number: 1\n",
      "batch number: 2\n",
      "batch number: 3\n",
      "batch number: 4\n",
      "batch number: 5\n",
      "batch number: 6\n",
      "batch number: 7\n",
      "batch number: 8\n",
      "batch number: 9\n",
      "batch number: 10\n",
      "batch number: 11\n",
      "batch number: 12\n",
      "batch number: 13\n",
      "batch number: 14\n",
      "batch number: 15\n",
      "batch number: 16\n",
      "batch number: 17\n",
      "batch number: 18\n",
      "batch number: 19\n",
      "batch number: 20\n",
      "batch number: 21\n",
      "batch number: 22\n",
      "batch number: 23\n"
     ]
    }
   ],
   "source": [
    "main(args, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc934ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--data', 'kinship', '--model', 'ggnn_distmult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-28 14:48:40.200632 (WARNING): Pipeline path /Users/gaohanning/.data/kinship already exist. This pipeline may overwrite data in this path!\n",
      "2021-07-28 14:48:40.201407 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab\n",
      "2021-07-28 14:48:40.215655 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e1\n",
      "2021-07-28 14:48:40.245011 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel\n",
      "2021-07-28 14:48:40.246089 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel_eval\n",
      "2021-07-28 14:48:40.252134 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2\n",
      "2021-07-28 14:48:40.269514 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi1\n",
      "2021-07-28 14:48:40.276675 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi2\n",
      "[21200, 10400, 40000, 200, 120000, 120000, 600, 600]\n",
      "313000\n",
      "2021-07-28 14:49:08.117566 (INFO): Total epoch time: 0:00:21\n",
      "2021-07-28 14:49:08.172978 (INFO): \n",
      "\n",
      "2021-07-28 14:49:08.192912 (INFO): ########################################\n",
      "2021-07-28 14:49:08.192974 (INFO):           COMPLETED EPOCH: 1                              \n",
      "2021-07-28 14:49:08.193001 (INFO): train Loss: 0.69279\t99% CI: (0.69262, 0.69296), n=23\n",
      "2021-07-28 14:49:08.193030 (INFO): ########################################\n",
      "2021-07-28 14:49:08.193053 (INFO): \n",
      "\n",
      "2021-07-28 14:49:27.154610 (INFO): Total epoch time: 0:00:18\n",
      "2021-07-28 14:49:27.221643 (INFO): \n",
      "\n",
      "2021-07-28 14:49:27.263760 (INFO): ########################################\n",
      "2021-07-28 14:49:27.263874 (INFO):           COMPLETED EPOCH: 2                              \n",
      "2021-07-28 14:49:27.263907 (INFO): train Loss: 0.68551\t99% CI: (0.68274, 0.68828), n=23\n",
      "2021-07-28 14:49:27.263932 (INFO): ########################################\n",
      "2021-07-28 14:49:27.263953 (INFO): \n",
      "\n",
      "2021-07-28 14:49:39.734922 (INFO): Total epoch time: 0:00:12\n",
      "2021-07-28 14:49:39.800238 (INFO): \n",
      "\n",
      "2021-07-28 14:49:39.800854 (INFO): ########################################\n",
      "2021-07-28 14:49:39.801358 (INFO):           COMPLETED EPOCH: 3                              \n",
      "2021-07-28 14:49:39.801394 (INFO): train Loss: 0.61095\t99% CI: (0.5898, 0.63211), n=23\n",
      "2021-07-28 14:49:39.801851 (INFO): ########################################\n",
      "2021-07-28 14:49:39.801934 (INFO): \n",
      "\n",
      "2021-07-28 14:49:39.858440 (INFO): \n",
      "2021-07-28 14:49:39.858823 (INFO): --------------------------------------------------\n",
      "2021-07-28 14:49:39.858865 (INFO): dev_evaluation\n",
      "2021-07-28 14:49:39.858891 (INFO): --------------------------------------------------\n",
      "2021-07-28 14:49:39.877623 (INFO): \n",
      "2021-07-28 14:50:15.437088 (INFO): Hits left @1: 0.0146484375\n",
      "2021-07-28 14:50:15.437421 (INFO): Hits right @1: 0.0078125\n",
      "2021-07-28 14:50:15.437563 (INFO): Hits @1: 0.01123046875\n",
      "2021-07-28 14:50:15.437658 (INFO): Hits left @2: 0.021484375\n",
      "2021-07-28 14:50:15.437752 (INFO): Hits right @2: 0.0234375\n",
      "2021-07-28 14:50:15.437877 (INFO): Hits @2: 0.0224609375\n",
      "2021-07-28 14:50:15.438000 (INFO): Hits left @3: 0.033203125\n",
      "2021-07-28 14:50:15.438092 (INFO): Hits right @3: 0.03515625\n",
      "2021-07-28 14:50:15.438218 (INFO): Hits @3: 0.0341796875\n",
      "2021-07-28 14:50:15.438309 (INFO): Hits left @4: 0.0517578125\n",
      "2021-07-28 14:50:15.438399 (INFO): Hits right @4: 0.0478515625\n",
      "2021-07-28 14:50:15.438526 (INFO): Hits @4: 0.0498046875\n",
      "2021-07-28 14:50:15.438925 (INFO): Hits left @5: 0.0615234375\n",
      "2021-07-28 14:50:15.439024 (INFO): Hits right @5: 0.060546875\n",
      "2021-07-28 14:50:15.439164 (INFO): Hits @5: 0.06103515625\n",
      "2021-07-28 14:50:15.439256 (INFO): Hits left @6: 0.0673828125\n",
      "2021-07-28 14:50:15.439348 (INFO): Hits right @6: 0.0771484375\n",
      "2021-07-28 14:50:15.439470 (INFO): Hits @6: 0.072265625\n",
      "2021-07-28 14:50:15.439561 (INFO): Hits left @7: 0.0791015625\n",
      "2021-07-28 14:50:15.439651 (INFO): Hits right @7: 0.08984375\n",
      "2021-07-28 14:50:15.439774 (INFO): Hits @7: 0.08447265625\n",
      "2021-07-28 14:50:15.439865 (INFO): Hits left @8: 0.091796875\n",
      "2021-07-28 14:50:15.440201 (INFO): Hits right @8: 0.10546875\n",
      "2021-07-28 14:50:15.440332 (INFO): Hits @8: 0.0986328125\n",
      "2021-07-28 14:50:15.440425 (INFO): Hits left @9: 0.1044921875\n",
      "2021-07-28 14:50:15.440516 (INFO): Hits right @9: 0.1142578125\n",
      "2021-07-28 14:50:15.440641 (INFO): Hits @9: 0.109375\n",
      "2021-07-28 14:50:15.440733 (INFO): Hits left @10: 0.12109375\n",
      "2021-07-28 14:50:15.440823 (INFO): Hits right @10: 0.1259765625\n",
      "2021-07-28 14:50:15.440951 (INFO): Hits @10: 0.12353515625\n",
      "2021-07-28 14:50:15.441084 (INFO): Mean rank left: 46.4462890625\n",
      "2021-07-28 14:50:15.441213 (INFO): Mean rank right: 46.0302734375\n",
      "2021-07-28 14:50:15.441668 (INFO): Mean rank: 46.23828125\n",
      "2021-07-28 14:50:15.441821 (INFO): Mean reciprocal rank left: 0.05899021302768066\n",
      "2021-07-28 14:50:15.442159 (INFO): Mean reciprocal rank right: 0.057283748325925525\n",
      "2021-07-28 14:50:15.442683 (INFO): Mean reciprocal rank: 0.05813698067680309\n",
      "saving best model to saved_models/kinship_ggnn_distmult_0.2_0.25.model\n",
      "2021-07-28 14:50:15.534422 (INFO): \n",
      "2021-07-28 14:50:15.578097 (INFO): --------------------------------------------------\n",
      "2021-07-28 14:50:15.578829 (INFO): test_evaluation\n",
      "2021-07-28 14:50:15.578867 (INFO): --------------------------------------------------\n",
      "2021-07-28 14:50:15.578891 (INFO): \n",
      "2021-07-28 14:50:50.628980 (INFO): Hits left @1: 0.01171875\n",
      "2021-07-28 14:50:50.629454 (INFO): Hits right @1: 0.0078125\n",
      "2021-07-28 14:50:50.629588 (INFO): Hits @1: 0.009765625\n",
      "2021-07-28 14:50:50.629681 (INFO): Hits left @2: 0.0205078125\n",
      "2021-07-28 14:50:50.629769 (INFO): Hits right @2: 0.0224609375\n",
      "2021-07-28 14:50:50.636246 (INFO): Hits @2: 0.021484375\n",
      "2021-07-28 14:50:50.636703 (INFO): Hits left @3: 0.0361328125\n",
      "2021-07-28 14:50:50.636807 (INFO): Hits right @3: 0.033203125\n",
      "2021-07-28 14:50:50.636937 (INFO): Hits @3: 0.03466796875\n",
      "2021-07-28 14:50:50.637054 (INFO): Hits left @4: 0.044921875\n",
      "2021-07-28 14:50:50.648340 (INFO): Hits right @4: 0.0458984375\n",
      "2021-07-28 14:50:50.648509 (INFO): Hits @4: 0.04541015625\n",
      "2021-07-28 14:50:50.648606 (INFO): Hits left @5: 0.05859375\n",
      "2021-07-28 14:50:50.648693 (INFO): Hits right @5: 0.0537109375\n",
      "2021-07-28 14:50:50.648831 (INFO): Hits @5: 0.05615234375\n",
      "2021-07-28 14:50:50.648928 (INFO): Hits left @6: 0.0732421875\n",
      "2021-07-28 14:50:50.649012 (INFO): Hits right @6: 0.0693359375\n",
      "2021-07-28 14:50:50.649127 (INFO): Hits @6: 0.0712890625\n",
      "2021-07-28 14:50:50.649207 (INFO): Hits left @7: 0.0791015625\n",
      "2021-07-28 14:50:50.649288 (INFO): Hits right @7: 0.0849609375\n",
      "2021-07-28 14:50:50.649400 (INFO): Hits @7: 0.08203125\n",
      "2021-07-28 14:50:50.649480 (INFO): Hits left @8: 0.08984375\n",
      "2021-07-28 14:50:50.649604 (INFO): Hits right @8: 0.0947265625\n",
      "2021-07-28 14:50:50.649731 (INFO): Hits @8: 0.09228515625\n",
      "2021-07-28 14:50:50.649817 (INFO): Hits left @9: 0.103515625\n",
      "2021-07-28 14:50:50.649897 (INFO): Hits right @9: 0.103515625\n",
      "2021-07-28 14:50:50.650009 (INFO): Hits @9: 0.103515625\n",
      "2021-07-28 14:50:50.650096 (INFO): Hits left @10: 0.1181640625\n",
      "2021-07-28 14:50:50.650186 (INFO): Hits right @10: 0.1171875\n",
      "2021-07-28 14:50:50.650305 (INFO): Hits @10: 0.11767578125\n",
      "2021-07-28 14:50:50.650438 (INFO): Mean rank left: 46.85546875\n",
      "2021-07-28 14:50:50.650557 (INFO): Mean rank right: 46.4365234375\n",
      "2021-07-28 14:50:50.650744 (INFO): Mean rank: 46.64599609375\n",
      "2021-07-28 14:50:50.651291 (INFO): Mean reciprocal rank left: 0.05669613487236841\n",
      "2021-07-28 14:50:50.652021 (INFO): Mean reciprocal rank right: 0.05514747716226583\n",
      "2021-07-28 14:50:50.652247 (INFO): Mean reciprocal rank: 0.05592180601731712\n",
      "2021-07-28 14:51:03.330250 (INFO): Total epoch time: 0:00:12\n",
      "2021-07-28 14:51:03.331445 (INFO): \n",
      "\n",
      "2021-07-28 14:51:03.331487 (INFO): ########################################\n",
      "2021-07-28 14:51:03.331516 (INFO):           COMPLETED EPOCH: 4                              \n",
      "2021-07-28 14:51:03.331543 (INFO): train Loss: 0.36278\t99% CI: (0.32098, 0.40457), n=23\n",
      "2021-07-28 14:51:03.331564 (INFO): ########################################\n",
      "2021-07-28 14:51:03.331584 (INFO): \n",
      "\n",
      "2021-07-28 14:51:15.984243 (INFO): Total epoch time: 0:00:12\n",
      "2021-07-28 14:51:15.985134 (INFO): \n",
      "\n",
      "2021-07-28 14:51:15.985177 (INFO): ########################################\n",
      "2021-07-28 14:51:15.985203 (INFO):           COMPLETED EPOCH: 5                              \n",
      "2021-07-28 14:51:15.985225 (INFO): train Loss: 0.21907\t99% CI: (0.20273, 0.2354), n=23\n",
      "2021-07-28 14:51:15.985244 (INFO): ########################################\n",
      "2021-07-28 14:51:15.985264 (INFO): \n",
      "\n",
      "2021-07-28 14:51:15.985396 (INFO): \n",
      "2021-07-28 14:51:15.985423 (INFO): --------------------------------------------------\n",
      "2021-07-28 14:51:15.985444 (INFO): dev_evaluation\n",
      "2021-07-28 14:51:15.985463 (INFO): --------------------------------------------------\n",
      "2021-07-28 14:51:15.985482 (INFO): \n"
     ]
    }
   ],
   "source": [
    "main(args, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dce20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
