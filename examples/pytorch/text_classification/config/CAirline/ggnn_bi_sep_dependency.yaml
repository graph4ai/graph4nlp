# Data
dataset: 'CAirline'
root_dir: 'examples/pytorch/text_classification/data/CAirline'
out_dir: 'examples/pytorch/text_classification/out/CAirline/ggnn_bi_sep_dependency_ckpt'

emb_strategy: 'bert_bilstm'
# Graph construction
graph_construction_args:
  graph_construction_share:
    graph_name: 'dependency'
    root_dir: 'examples/pytorch/text_classification/data/trec'
    # topology_subdir: 'dependency_graph'
    thread_number: 10
    port: 9000
    timeout: 15000

  graph_construction_private:
    edge_strategy: 'homogeneous'
    merge_strategy: 'tailhead'
    sequential_link: true
    as_node: false
    dynamic_init_graph_name: null # initial graph construction type ('line', 'dependency', 'constituency', 'ie')

# Dynamic graph construction
gl_metric_type: null # similarity metric type for dynamic graph construction ('weighted_cosine', 'attention', 'rbf_kernel', 'cosine')
gl_epsilon: null # epsilon for graph sparsification
gl_top_k: null # top k for graph sparsification
gl_num_heads: 1 # num of heads for dynamic graph construction
gl_num_hidden: 300 # number of hidden units for dynamic graph construction
gl_smoothness_ratio: null # smoothness ratio for graph regularization loss
gl_sparsity_ratio: null # sparsity ratio for graph regularization loss
gl_connectivity_ratio: null # connectivity ratio for graph regularization loss
init_adj_alpha: 0.8 # alpha ratio for combining initial graph adjacency matrix


# Graph embedding construction
word_dropout: 0.2 # word embedding dropout
#  - 0.4
#  - 0.3
#  - 0.2
rnn_dropout: 0.2 # RNN dropout
#  - 0.1
#  - 0.2
no_fix_word_emb: true # Not fix pretrained word embeddings (default: false)
#    - true
#    - false
node_edge_emb_strategy: 'mean' # node edge embedding strategy for graph embedding construction ('mean', 'lstm', 'gru', 'bilstm' and 'bigru')
seq_info_encode_strategy: 'bilstm' # sequence info encoding strategy for graph embedding construction ('none', 'lstm', 'gru', 'bilstm' and 'bigru')


# GNN
gnn: 'ggnn'
gnn_direction_option: 'bi_sep' # GNN direction type ('undirected', 'bi_sep', 'bi_fuse')
gnn_num_layers: 3 # number of GNN layers
#  - 2
#  - 3
#  - 4
num_hidden: 256 # number of hidden units
#  - 128
#  - 256
#  - 300
graph_pooling: 'avg_pool' # graph pooling ('avg_pool', 'max_pool')
max_pool_linear_proj: false # use linear projectioni for max pooling
gnn_dropout: 0.2 # GNN input feature dropout
#    - 0.2
#    - 0.3
#    - 0.5
# GAT
gat_attn_dropout: 0.3 # GAT attention dropout
gat_negative_slope: 0.2 # the negative slope of leaky relu
gat_num_heads: 1 # number of hidden attention heads
#  - 1
#  - 2
gat_num_out_heads: 2 # number of output attention heads
gat_residual: false # use gat_residual connection
# GraphSAGE
graphsage_aggreagte_type: null # graphsage aggreagte type ('mean', 'gcn', 'pool', 'lstm')


# Training
seed: 1234
batch_size: 50 # batch size
#  - 50
#  - 32
epochs: 500 # number of maximal training epochs
patience: 10
lr: 0.003 # learning rate
#      - 0.003
#      - 0.002
#      - 0.001
lr_patience: 2
lr_reduce_factor: 0.5
num_workers: 8 # number of data loader workers


gpu: -1
no_cuda: false
