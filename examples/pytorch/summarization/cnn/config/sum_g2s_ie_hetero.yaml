# Users can import user customized yaml files or library provided default yaml files
includes: []
  # - $/configs/defaults.yaml
  # In the above example path, '$/' indicates the library directory which contains the config folder

preprocessing_args:
  top_word_vocab: 70000
  word_emb_size: 300
  share_vocab: True
  min_word_freq: 1


model_args:
  graph_construction_name: "ie"
  graph_embedding_name: "rgcn"
  decoder_name: "stdrnn"

  graph_construction_args:
    graph_construction_share:
      root_dir: "examples/pytorch/summarization/cnn"
#      topology_subdir: 'IEGraph_3w'
#      topology_subdir: 'IEGraph_stanza'
#      topology_subdir: 'IEGraph_stanza_hetero'
      topology_subdir: 'IEGraph_stanza_hetero_1w'
      thread_number: 35

      nlp_processor_args:
        name: "stanza"
        args:
          corenlp_dir: "/home/gaohanning/graph4nlp/stanford-corenlp-4.0.0f"
          endpoint: "http://localhost:9001"
          memory: "4G"
          annotators: ["tokenize, ssplit, pos, lemma, ner, parse, coref, openie"]
          properties:
            tokenize.options:
              splitHyphenated: True
              normalizeParentheses: True
              normalizeOtherBrackets: True
            tokenize.whitespace: False
            ssplit.isOneSentence: False
            openie.triple.strict: True

    graph_construction_private:
      edge_strategy: "heterogeneous"
      merge_strategy: null
      sequential_link: true
      as_node: false

  graph_initialization_args:
    input_size: 300
    hidden_size: 300
    word_dropout: 0.2
    rnn_dropout: 0.3
    fix_bert_emb: false
    fix_word_emb: false
    embedding_style:
      single_token_item: false
      emb_strategy: "w2v_bilstm"
      num_rnn_layers: 1
      bert_model_name: null
      bert_lower_case: null

  graph_embedding_args:
    graph_embedding_share:
      num_layers: 3
      input_size: 300
      hidden_size: 300
      output_size: 300
      direction_option: "bi_fuse"
      feat_drop: 0.2

    graph_embedding_private:
      num_rels: 2

  decoder_args:
    rnn_decoder_share:
      rnn_type: "lstm"
      input_size: 300
      hidden_size: 512
      rnn_emb_input_size: 300
      use_copy: false
      use_coverage: false
      graph_pooling_strategy: "max"
      attention_type: "sep_diff_encoder_type"
      fuse_strategy: "concatenate"
      dropout: 0.3

    rnn_decoder_private:
      max_decoder_step: 100
      node_type_num: null
      tgt_emb_as_output_layer: true


training_args:
  batch_size: 32 # batch size
  epochs: 100 # number of maximal training epochs
  grad_clipping: 10
  early_stop_metric: 'ROUGE'
  patience: 10
  lr: 0.0005 # learning rate
  lr_patience: 2
  lr_reduce_factor: 0.5
  coverage_loss_ratio: 0 # coverage loss ratio


inference_args:
  beam_size: 3


evaluation_args:
  # Metrics for evaluation
  metrics: ['ROUGE']


checkpoint_args:
  out_dir: 'out/cnn/rgcn_bi_fuse_l2_ie_hetero_ckpt'


env_args:
  seed: 1234
  gpu: 3
  no_cuda: false
  num_workers: 0